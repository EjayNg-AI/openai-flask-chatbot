[
  {
    "content": "You are an expert in coding. Write code that is clean, efficient, modular, and well-documented. Ensure all code block are encased with triple backticks.",
    "role": "system"
  },
  {
    "content": "Please refer to the attached project files which form part of a full stack web application.\n\nWhen sanitizing HTML tags, one issue is unresolved. The frontend places <pre> ... </pre> tags upon opening a new page. In particular, if a file is being loaded, the markdown contents retrieved from local storage is placed within those tags. This behavior is *intentional*.\n\nHowever, upon subsequent conversation turns, sanitization turns those <pre> ... </pre> tags into &lt;pre&gt; ... &lt;/pre&gt; -- and this behavior does not align with the original intent of the pre tags.\n\nI wish the sanization behavior to be retain, but also want to resolve this remaining issue. Please suggest solutions.\n---\n\nFILE: templates/index.html\n\n<!DOCTYPE html>\n<html lang=\"en\">\n  <head>\n    <meta charset=\"UTF-8\" />\n    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\" />\n    <title>Chatbot powered by State of the Art LLMs</title>\n\n    <script>\n      window.MathJax = {\n        tex: {\n          inlineMath: [\n            [\"$\", \"$\"],\n            [\"\\\\(\", \"\\\\)\"],\n          ],\n          displayMath: [\n            [\"$$\", \"$$\"],\n            [\"\\\\[\", \"\\\\]\"],\n          ],\n        },\n        options: {\n          skipHtmlTags: [\"script\", \"noscript\", \"style\", \"textarea\", \"pre\"],\n        },\n      };\n    </script>\n\n    <script src=\"https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js\"></script>\n    <script src=\"https://cdn.jsdelivr.net/npm/marked/marked.min.js\"></script>\n    <link\n      rel=\"stylesheet\"\n      href=\"https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.7.0/styles/default.min.css\"\n    />\n    <script src=\"https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.7.0/highlight.min.js\"></script>\n    <link\n      href=\"https://fonts.googleapis.com/css2?family=Roboto:wght@400;700&display=swap\"\n      rel=\"stylesheet\"\n    />\n\n    <style>\n      body {\n        font-family: \"Roboto\", sans-serif;\n        margin: 0;\n        padding: 0;\n        display: flex;\n        flex-direction: column;\n        height: 100vh;\n        overflow: hidden;\n      }\n\n      .control-panel {\n        display: flex;\n        gap: 10px;\n        padding: 20px;\n        background-color: #f0f0f0;\n        border-bottom: 1px solid #ddd;\n      }\n\n      .query-input {\n        font-size: 16px;\n        flex: 1;\n        resize: none;\n        line-height: 1.2;\n        min-height: 10px;\n        max-height: 70vh;\n        overflow-y: auto;\n        transition: height 0.2s ease;\n        padding: 2.3px;\n        border: 1px solid #ddd;\n        border-radius: 4px;\n      }\n\n      .button {\n        padding: 10px 20px;\n        border: none;\n        border-radius: 4px;\n        cursor: pointer;\n        font-weight: bold;\n        width: 80px;\n        /* Fixed width */\n        height: 60px;\n        /* Fixed height */\n        flex-shrink: 0;\n        /* Prevent buttons from shrinking */\n      }\n\n      .toggle-dark-mode {\n        background-color: #008080;\n        color: white;\n      }\n\n      .toggle-dark-mode:hover {\n        background-color: #005f5f;\n      }\n\n      .sanitize-btn {\n        background-color: #0066cc;\n        color: white;\n      }\n\n      .sanitize-btn:hover {\n        background-color: #004999;\n      }\n\n      .copy-prompt-btn:hover {\n        background-color: #ff9800;\n        color: white;\n      }\n\n      .copy-prompt-btn {\n        background-color: #f57c00;\n        color: white;\n      }\n\n      .load-btn {\n        background-color: #aa4444;\n        color: white;\n      }\n\n      .load-btn:hover {\n        background-color: #dd7777;\n        color: white;\n      }\n\n      .save-btn {\n        background-color: #444444;\n        color: white;\n      }\n\n      .save-btn:hover {\n        background-color: #777777;\n        color: white;\n      }\n\n      .submit-btn {\n        background-color: #4caf50;\n        color: white;\n      }\n\n      .submit-btn:hover {\n        background-color: #45df49;\n        color: white;\n      }\n\n      .clearscreen-btn {\n        background-color: #550055;\n        color: white;\n      }\n\n      .clearscreen-btn:hover {\n        background-color: #990099;\n        color: white;\n      }\n\n      .close-btn {\n        background-color: #0000aa;\n        color: white;\n      }\n\n      .close-btn:hover {\n        background-color: #0055ff;\n        color: white;\n      }\n\n      .file-viewer-btn {\n        background-color: #aa0044;\n        color: white;\n      }\n\n      .file-viewer-btn:hover {\n        background-color: #cc0066;\n        color: white;\n      }\n\n      .consolidate-btn {\n        background-color: #444444;\n        color: white;\n      }\n\n      .consolidate-btn:hover {\n        background-color: #777777;\n        color: white;\n      }\n\n      .reset-btn {\n        background-color: #f44336;\n        color: white;\n      }\n\n      .reset-btn:hover {\n        background-color: #ff0099;\n        color: white;\n      }\n\n      .conversation-turn {\n        margin-bottom: 20px;\n        padding-bottom: 20px;\n        border-bottom: 1px solid #eee;\n      }\n\n      .query-container {\n        padding: 20px;\n        background-color: #f0f0f0;\n        border-bottom: 1px solid #ddd;\n      }\n\n      .container {\n        display: flex;\n        flex: 1;\n        overflow: hidden;\n      }\n\n      .editor,\n      .preview {\n        width: 50%;\n        padding: 20px;\n        box-sizing: border-box;\n        overflow: auto;\n        min-width: 0;\n      }\n\n      .editor {\n        border-right: 1px solid #ddd;\n        background-color: #f9f9f9;\n      }\n\n      #markdown-input {\n        width: 100%;\n        height: 100%;\n        resize: none;\n        border: none;\n        outline: none;\n        font-size: 16px;\n        font-family: \"Courier New\", Courier, monospace;\n      }\n\n      .preview {\n        background-color: #fff;\n      }\n\n      pre {\n        position: relative;\n        background-color: #f0f0f0;\n        padding: 15px;\n        overflow: auto;\n        border-radius: 5px;\n      }\n\n      pre button.copy-btn {\n        position: absolute;\n        top: 10px;\n        right: 10px;\n        background-color: #4caf50;\n        border: none;\n        color: white;\n        padding: 5px 10px;\n        cursor: pointer;\n        border-radius: 3px;\n        font-size: 12px;\n      }\n\n      blockquote {\n        border-left: 5px solid #ccc;\n        padding-left: 15px;\n        color: #666;\n        margin: 1em 0;\n      }\n\n      .model-controls {\n        display: flex;\n        align-items: center;\n        gap: 5px;\n        padding: 5px;\n        background-color: #ffffff;\n        border-bottom: 1px solid #000000;\n      }\n\n      .model-controls .drop-down-menu {\n        display: flex;\n        align-items: center;\n        gap: 5px;\n      }\n\n      /* ─── Light‑mode fixes for the left “file‑browser” panel ─── */\n      body:not(.dark-mode) #file-browser {\n        background-color: #fafafa;\n        color: #111;\n      }\n\n      body:not(.dark-mode) #directory-tree .directory-entry,\n      body:not(.dark-mode) #directory-tree .toggle-triangle {\n        color: #222;\n      }\n\n      body:not(.dark-mode) #directory-tree a.file-link {\n        color: #007bff;\n      }\n\n      body:not(.dark-mode) #directory-tree a.file-link:hover {\n        color: #0056b3;\n      }\n\n      #markdown-input {\n        white-space: pre-wrap;\n        word-break: break-word;\n        overflow-wrap: anywhere;\n      }\n\n      #chat-panel,\n      .container,\n      .editor,\n      .preview {\n        min-width: 0;\n      }\n\n      .preview pre {\n        overflow-x: auto;\n      }\n\n      /* ---------------- Dark Mode Styles ---------------- */\n      /* When the body receives the \"dark-mode\" class the following rules apply */\n      body.dark-mode {\n        background-color: #121212;\n        color: white;\n        /* General text color */\n      }\n\n      /* Override background and text for control panels & header areas */\n      body.dark-mode .control-panel,\n      body.dark-mode .model-controls {\n        background-color: #181818;\n        color: white;\n      }\n\n      /* Editor and preview areas use near‑black backgrounds with white text */\n      body.dark-mode .editor,\n      body.dark-mode .preview,\n      body.dark-mode textarea {\n        background-color: #121212;\n        color: white;\n      }\n\n      /* Code blocks use a different dark background for contrast */\n      body.dark-mode pre {\n        background-color: #2e2e2e;\n        color: white;\n      }\n\n      /* ---------------- End Dark Mode Styles ---------------- */\n\n      /* ———— new two‐pane layout ———— */\n      #content-wrapper {\n        display: flex;\n        flex: 1;\n        overflow: hidden;\n      }\n\n      #file-browser {\n        flex: 0 0 240px;\n        height: 100%;\n        min-width: 120px;\n        border-right: 1px solid #ccc;\n        padding: 8px;\n        box-sizing: border-box;\n        overflow-y: auto;\n      }\n\n      #file-browser button {\n        margin: 3px 0;\n        width: 100%;\n        padding: 6px 8px;\n        font-size: 0.85em;\n      }\n\n      #directory-tree .node {\n        margin-left: 20px;\n      }\n\n      #directory-tree .toggle-triangle {\n        display: inline-block;\n        width: 1em;\n        text-align: center;\n        cursor: pointer;\n        user-select: none;\n        margin-right: 5px;\n      }\n\n      #directory-tree .directory-entry {\n        font-weight: bold;\n      }\n\n      #directory-tree a.file-link {\n        display: block;\n        color: #00cc00;\n        text-decoration: none;\n        margin: 2px 0;\n      }\n\n      #directory-tree a.file-link:hover {\n        text-decoration: underline;\n        color: #00ff00;\n      }\n\n      /* pull your old .container into the right pane */\n      #chat-panel {\n        flex: 1;\n        display: flex;\n        flex-direction: column;\n      }\n    </style>\n  </head>\n\n  <body>\n    <!-- … your model‐controls & control‐panel … -->\n\n    <div id=\"content-wrapper\">\n      <!-- LEFT: file‐browser -->\n      <div id=\"file-browser\">\n        <button id=\"open-folder\">Open Directory</button>\n        <button id=\"refresh\">Refresh</button>\n        <div id=\"directory-tree\"></div>\n      </div>\n\n      <!-- RIGHT: your existing chat UI -->\n      <div id=\"chat-panel\">\n        <div class=\"model-controls\">\n          <div class=\"drop-down-menu\">\n            <form>\n              <label for=\"language-model\">Select Language Model:</label>\n              <select id=\"language-model\">\n                <option value=\"chatgpt-4o-latest\">chatgpt-4o-latest</option>\n                <option value=\"gpt-4o\">gpt-4o</option>\n                <option value=\"gpt-4.1\">gpt-4.1</option>\n                <option value=\"o1-pro\">o1-pro</option>\n                <option value=\"o3\">o3</option>\n                <option value=\"o4-mini\">o4-mini</option>\n                <option value=\"o3-mini\">o3-mini</option>\n                <option value=\"claude-3-7-sonnet-20250219\">\n                  claude-3-7-sonnet-20250219\n                </option>\n                <option value=\"gemini-2.5-flash-preview-04-17\">\n                  gemini-2.5-flash-preview-04-17\n                </option>\n                <option value=\"gemini-2.5-pro-preview-03-25\">\n                  gemini-2.5-pro-preview-03-25\n                </option>\n              </select>\n            </form>\n            <button id=\"model_choice\">Select Model</button>\n          </div>\n          <div>\n            <label style=\"margin-left: 10px\">\n              <input type=\"checkbox\" id=\"claude-thinking\" />\n              Claude Extended Thinking\n            </label>\n          </div>\n          <div>\n            <label style=\"margin-left: 10px\">\n              <input type=\"checkbox\" id=\"math-render\" />\n              Escape Output & Render Math\n            </label>\n          </div>\n          <div>\n            <button id=\"file-viewer-btn\" class=\"file-viewer-btn\">\n              File Viewer\n            </button>\n          </div>\n          <div style=\"display: flex; align-items: center; gap: 5px\">\n            <button id=\"consolidate-btn\" class=\"consolidate-btn\">\n              Consolidate\n            </button>\n            <select id=\"consolidate-option\">\n              <option value=\"fully_updated\">Fully Updated Files</option>\n              <option value=\"refactor\">Refactor Code</option>\n              <option value=\"critique\">Critique Code</option>\n              <option value=\"synopsis\">Synopsis of Project</option>\n            </select>\n          </div>\n          <div>\n            <button id=\"sanitize-btn\" class=\"sanitize-btn\">\n              Sanitize Scripts\n            </button>\n          </div>\n          <div>\n            <button id=\"toggle-dark-mode\" class=\"toggle-dark-mode\">\n              Toggle Dark Mode\n            </button>\n          </div>\n        </div>\n\n        <div class=\"control-panel\">\n          <textarea\n            type=\"text\"\n            id=\"query-input\"\n            class=\"query-input\"\n            placeholder=\"Enter your question...\"\n          ></textarea>\n          <button id=\"copy-prompt-btn\" class=\"button copy-prompt-btn\">\n            Copy Prompt\n          </button>\n          <button id=\"submit-btn\" class=\"button submit-btn\">Submit</button>\n          <button id=\"clearscreen-btn\" class=\"button clearscreen-btn\">\n            Clear Screen\n          </button>\n          <button id=\"load-btn\" class=\"button load-btn\">Load</button>\n          <button id=\"save-btn\" class=\"button save-btn\">Save</button>\n          <button id=\"reset-btn\" class=\"button reset-btn\">Reset</button>\n          <button id=\"shutdown-btn\" class=\"button close-btn\">Close</button>\n        </div>\n        <div class=\"container\">\n          <div class=\"editor\">\n            <textarea id=\"markdown-input\"></textarea>\n          </div>\n          <div class=\"preview\" id=\"preview\"></div>\n        </div>\n      </div>\n    </div>\n\n    <script>\n      hljs.highlightAll();\n\n      const dropdown_menu = document.getElementById(\"language-model\");\n      const queryInput = document.getElementById(\"query-input\");\n      const copyPromptBtn = document.getElementById(\"copy-prompt-btn\");\n      const submitBtn = document.getElementById(\"submit-btn\");\n      const clear_scr = document.getElementById(\"clearscreen-btn\");\n      const loadBtn = document.getElementById(\"load-btn\");\n      const SaveBtn = document.getElementById(\"save-btn\");\n      const closeBtn = document.getElementById(\"shutdown-btn\");\n      const resetBtn = document.getElementById(\"reset-btn\");\n      const markdownInput = document.getElementById(\"markdown-input\");\n      const preview = document.getElementById(\"preview\");\n      const choose_model = document.getElementById(\"model_choice\");\n      const claudeThinking = document.getElementById(\"claude-thinking\");\n      const MathRender = document.getElementById(\"math-render\");\n\n      submitBtn.disabled = true;\n      SaveBtn.disabled = true;\n      choose_model.disabled = true;\n      clear_scr.disabled = true;\n\n      let uniqueId;\n      let model_name;\n      let streaming_flag = true;\n      let claude_thinking_flag = false;\n      let math_render_flag = false;\n      let consolidate_flag = false;\n      let currentFileContents = \"\";\n      let consolidate_option = \"fully_updated\";\n      let prompt_consolidate =\n        \"In this conversation, I initially provided project code files. Based on all the feedback, suggestions, and revisions discussed so far, please output the full, updated set of project files, organized and ready to copy into my development environment. Reflect all code, structural, and dependency updates. Ensure that the final output is fully integrated, consistent, and ready for use.\";\n\n      let refactoringPrompt = `Objective: Refactor and clean up the existing codebase to improve its readability, maintainability, and modularity **without altering any existing functionality or user experience**.\n\nStrict Constraints:\n1. Do not introduce any changes in logic, business rules, or user-facing behavior.\n2. Preserve exact input/output behavior, side-effects, UI layout/flow, and performance characteristics (within a negligible margin).\n3. Do not remove or simplify code that appears redundant unless it has *zero* effect on behavior and is *provably unreachable or unnecessary*.\n\nGoals of Refactoring:\n- Improve structure, clarity, and cohesion of the codebase.\n- Reduce deeply nested logic, excessive coupling, and repetition.\n- Apply standard best practices (e.g., DRY, SRP, consistent naming, modularization).\n- Introduce helper functions, configuration files, or abstractions only if they increase clarity.\n- Document public interfaces, key functions, and assumptions where lacking.\n\nCode Coverage & Testing:\n- All changes must be accompanied by unit or integration tests where possible.\n- Validate that test results remain unchanged post-refactor. If no test suite exists, assist in generating regression tests from current behavior.\n\nDeliverables:\n- Refactored code with in-line comments on major changes or restructuring decisions.\n- A summary of the changes, including files touched, functions moved/extracted, patterns replaced, and any identified anti-patterns.\n- (Optional) A cleanup roadmap for future maintainability and architectural improvements.\n\nOutput:\n- Output one refactored file at a time, including the original file name and a summary of changes, recommendations and other notes.\n- Prompt the user to confirm the changes before proceeding to the next file.\n`;\n\n      let synopsisPrompt = `# Provide structured metadata of codebase\n\n**You are a senior software engineer and architect.** You have been provided with multiple source files in your current context. For **each** file, please extract and summarize the following metadata. Then, provide an **overall, high-level summary** of the entire codebase at the end.\n\n---\n\n## I. Codebase-Level Information\n\n1. **Directory Structure**\n   - Provide a tree-style or indented list of file and folder paths relative to the project root.\n\n2. **Programming Languages & Key Technologies**\n   - List the programming languages, frameworks, libraries, and technologies used across the codebase.\n\n3. **Entry Points & Execution Flow**\n   - Identify main entry points.\n   - Briefly describe how control flows through the application from start to finish.\n\n4. **High-Level Architecture Overview**\n   - Summarize the architectural style (e.g., monolithic, microservices, event-driven).\n   - Describe major components or modules and how they interact.\n\n---\n\n## II. File-Level Information (Repeat for Each File)\n\nFor each **source code file**, provide the following metadata:\n\n1. **File Path**  \n   - The full path (or best estimate if partial paths are given).  \n\n2. **File Type/Language**  \n   - Explicitly state the language or file type (e.g., Python, JavaScript, CSS, HTML, JSON).  \n\n3. **High-Level Summary/Purpose**  \n   - A concise (1–3 sentence) natural language description of what the file does or which feature/component it represents.  \n\n4. **Key Entities Defined**  \n   - A list of major classes, functions, methods, constants, variables, interfaces, or components.  \n\n5. **Dependencies (Imports/Requires)**  \n   - A list of other modules/files (from **within** this codebase) that this file imports or requires.  \n\n6. **Exports/Public Interface**  \n   - A list of entities (classes, functions, constants, etc.) that are publicly exposed and can be used by other files.  \n\n7. **Docstrings for Python Modules**  \n   - For *python modules*, provide docstrings for each class and function therein.\n\n---\n\n## Output Format\n\n- Format your final answer as a **Markdown document** with clear headings and subheadings.\n- First, provide a **\"Codebase-Level Information\"** section.\n- Then, list all **file-level** metadata (in whichever order the files appear or by directory order).\n- If any of the elements do not apply to a particular file, please omit them or mark them as “N/A.”  \n- Make sure to keep your headings and structure consistent.\n- Be **concise** but **complete**. Avoid repeating boilerplate. Capture purpose, semantics, and relationships, not just syntax.\n\n---\n\n`;\n\n      /**\n       * sanitizeText scans the given markdown text and escapes any HTML tags\n       * (e.g. <img>, <a>, <p> etc.) that appear outside of code blocks and\n       * that are not already escaped.\n       *\n       * This version recognizes code blocks fenced with either three or four backticks.\n       *\n       * @param {string} text - The markdown text to sanitize.\n       * @returns {string} - The sanitized text.\n       */\n      function sanitizeText(text) {\n        /**\n         * sanitizeInline processes a given text segment (outside fenced code blocks)\n         * and escapes any HTML tags found outside inline code blocks.\n         *\n         * @param {string} segment - A portion of text that might include inline code.\n         * @returns {string} - The segment with HTML tags normalized and escaped.\n         */\n        function sanitizeInline(segment) {\n          // Split the segment by inline code blocks wrapped in single backticks.\n          var inlineParts = segment.split(/(`[^`]*`)/g);\n          for (var i = 0; i < inlineParts.length; i++) {\n            // Process only parts that are not inline code.\n            if (!/^`[^`]*`$/.test(inlineParts[i])) {\n              inlineParts[i] = inlineParts[i].replace(\n                /<\\s*(\\/?)\\s*([\"']?)([a-zA-Z][a-zA-Z0-9-]*)([\"']?)([^>]*)>/g,\n                function (match, slash, openQuote, tagName, closeQuote, rest) {\n                  var trimmedRest = rest.trim();\n                  var normalizedTag =\n                    \"&lt;\" +\n                    (slash ? slash : \"\") +\n                    (openQuote ? openQuote : \"\") +\n                    tagName +\n                    (closeQuote ? closeQuote : \"\");\n                  if (trimmedRest === \"/\") {\n                    normalizedTag += trimmedRest + \"&gt;\";\n                  } else if (trimmedRest) {\n                    normalizedTag += \" \" + trimmedRest + \"&gt;\";\n                  } else {\n                    normalizedTag += \"&gt;\";\n                  }\n                  return normalizedTag;\n                }\n              );\n            }\n          }\n          return inlineParts.join(\"\");\n        }\n\n        // Updated regex to recognize code blocks with either 3 or 4 backticks.\n        var parts = text.split(/((?:`{3,4})[\\s\\S]*?(?:`{3,4}))/g);\n        for (var j = 0; j < parts.length; j++) {\n          // Only sanitize parts that are not identified as code blocks.\n          if (!/^(?:`{3,4})/.test(parts[j])) {\n            parts[j] = sanitizeInline(parts[j]);\n          }\n        }\n        return parts.join(\"\");\n      }\n\n      /**\n       * Unescapes HTML entities (&lt; and &gt;) inside fenced code blocks.\n       * This version uses a regex that captures the entire block, ensures that\n       * the fence markers are the same (by using a backreference), and then unescapes\n       * the content between the markers.\n       *\n       * @param {string} text - The input markdown text.\n       * @returns {string} - The text with HTML entities unescaped inside fenced code blocks.\n       */\n      function unescapeHtmlEntitiesInFencedCodeBlocks(text) {\n        // This regex matches a fence (3 or more backticks), followed by any content,\n        // and then the same fence again (the backreference \\1 ensures that the closing\n        // fence is exactly the same as the opening fence).\n        const codeBlockRegex = /(^`{3,})([\\s\\S]*?)(^\\1$)/gm;\n\n        return text.replace(\n          codeBlockRegex,\n          (match, fence, codeContent, closingFence) => {\n            // Unescape HTML entities only in the content of the code block.\n            const unescapedContent = codeContent\n              .replace(/&lt;/g, \"<\")\n              .replace(/&gt;/g, \">\");\n            return fence + unescapedContent + closingFence;\n          }\n        );\n      }\n\n      /*\n  // Example usage:\n  const originalMarkdown = \"Normal text <p>with tag</p>\\n\\n````\\n// Some code: &lt;div&gt;should be unescaped&lt;/div&gt;\\n// More code: &lt;span&gt;should not be escaped by sanitizeText&lt;/span&gt;\\n````\\n\\nMore text with <a href='#'>link</a>.\";\n  \n  // First, sanitize tags outside code blocks:\n  const sanitized = sanitizeText(originalMarkdown);\n  const finalText = unescapeHtmlEntitiesInFencedCodeBlocks(sanitized);\n  // finalText is now: \"Normal text &lt;p&gt;with tag&lt;/p&gt;\\n\\n````\\n// Some code: &lt;div&gt;should be unescaped&lt;/div&gt;\\n// More code: &lt;span&gt;should not be escaped by sanitizeText&lt;/span&gt;\\n````\\n\\nMore text with &lt;a href='#'&gt;link&lt;/a&gt;.\"\n  \n  console.log(\"Original:\\n\", originalMarkdown);\n  console.log(\"\\nSanitized:\\n\", sanitized);\n  console.log(\"\\nFinal Text (Restored inside code):\\n\", finalText);\n  */\n\n      /**\n       * sanitizeLeftPanel retrieves the current contents of the editor (left panel),\n       * sanitizes it (escaping any tags that are not already inside code blocks),\n       * updates the editor with the sanitized content, and then re-renders the preview.\n       */\n      function sanitizeLeftPanel() {\n        var editor = document.getElementById(\"markdown-input\");\n        if (!editor) {\n          console.error(\"Editor element (#markdown-input) not found.\");\n          return;\n        }\n        var originalText = editor.value;\n        var sanitizedText = sanitizeText(originalText);\n        if (sanitizedText !== originalText) {\n          editor.value = sanitizedText;\n          // Re-render the preview; the renderMarkdown function is defined elsewhere.\n          renderMarkdown(true);\n          alert(\n            \"Sanitization complete. Tags outside code blocks have been escaped.\"\n          );\n        } else {\n          alert(\"No unsanitized tags found outside code blocks.\");\n        }\n      }\n\n      // Attach the sanitizeLeftPanel function to the new button's click event.\n      document\n        .getElementById(\"sanitize-btn\")\n        .addEventListener(\"click\", sanitizeLeftPanel);\n\n      // When the 'Toggle Dark Mode' button is clicked, toggle the \"dark-mode\" class on the body element\n      document\n        .getElementById(\"toggle-dark-mode\")\n        .addEventListener(\"click\", function () {\n          document.body.classList.toggle(\"dark-mode\");\n        });\n\n      MathRender.addEventListener(\"change\", async () => {\n        math_render_flag = MathRender.checked;\n        // Send to backend\n        try {\n          await fetch(\"/update-math-render\", {\n            method: \"POST\",\n            headers: {\n              \"Content-Type\": \"application/json\",\n            },\n            body: JSON.stringify({\n              math_render_flag: math_render_flag,\n            }),\n          });\n          if (markdownInput) {\n            markdownInput.value += `\\n Math Rendering: ${math_render_flag} \\n`;\n          }\n        } catch (err) {\n          console.error(\"Failed to update backend:\", err);\n        }\n      });\n\n      claudeThinking.addEventListener(\"change\", async () => {\n        claude_thinking_flag = claudeThinking.checked;\n        // Send to backend\n        try {\n          await fetch(\"/update-claude-thinking\", {\n            method: \"POST\",\n            headers: {\n              \"Content-Type\": \"application/json\",\n            },\n            body: JSON.stringify({\n              claude_thinking_flag: claude_thinking_flag,\n            }),\n          });\n          if (markdownInput) {\n            markdownInput.value += `\\n Claude Extended Thinking: ${claude_thinking_flag} \\n`;\n          }\n        } catch (err) {\n          console.error(\"Failed to update backend:\", err);\n        }\n      });\n\n      document\n        .getElementById(\"file-viewer-btn\")\n        .addEventListener(\"click\", () => {\n          window.open(\"/filecontents\", \"_blank\");\n        });\n\n      document.getElementById(\"load-btn\").addEventListener(\"click\", () => {\n        // this will cause the Flask handler to pop up a native file chooser\n        window.open(\"/load\", \"_blank\");\n      });\n\n      document\n        .getElementById(\"consolidate-btn\")\n        .addEventListener(\"click\", () => {\n          const consolidateOption =\n            document.getElementById(\"consolidate-option\").value;\n\n          if (!uniqueId || !model_name || streaming_flag === undefined) {\n            alert(\"Missing required data.\");\n            return;\n          }\n\n          const params = new URLSearchParams({\n            uniqueId: uniqueId,\n            model_name: model_name,\n            streaming_flag: streaming_flag,\n            option: consolidateOption,\n          });\n\n          const url = `/consolidate?${params.toString()}`;\n          window.open(url, \"_blank\");\n        });\n\n      function updateFileContents(contents) {\n        currentFileContents = contents;\n        console.log(\"File contents updated in main window.\");\n      }\n\n      // Fetch token from the Flask backend\n      fetch(\"/get-token\")\n        .then((response) => response.json())\n        .then((data) => {\n          // Set the global uniqueId\n          uniqueId = data.token;\n          model_name = data.model_name;\n          streaming_flag = ![\"o1-pro\"].includes(model_name);\n          consolidate_flag = data.consolidate_conversation_flag;\n          consolidate_option = data.consolidate_option;\n          const initialMarkdown = data.markdown_content || \"\";\n          setTimeout(() => {\n            if (markdownInput) {\n              let sanitizedMarkdown = sanitizeText(initialMarkdown);\n              markdownInput.value =\n                \"<pre> \\n\" + sanitizedMarkdown + \"\\n </pre>\";\n              markdownInput.scrollTop = markdownInput.scrollHeight;\n              markdownInput.value += `\\n\\n Unique Session ID: ${uniqueId} \\n`;\n              markdownInput.value += `\\n Model: ${model_name} \\n`;\n              markdownInput.value += `\\n Streaming: ${streaming_flag} \\n`;\n              renderMarkdown(true);\n              submitBtn.disabled = false;\n              SaveBtn.disabled = false;\n              choose_model.disabled = false;\n              clear_scr.disabled = false;\n            }\n          }, 0);\n          if (consolidate_flag) {\n            if (consolidate_option === \"fully_updated\") {\n              queryInput.value =\n                \"In this conversation, I initially provided project code files. Based on all the feedback, suggestions, and revisions discussed so far, please output the full, updated set of project files, organized and ready to copy into my development environment. Reflect all code, structural, and dependency updates. Ensure that the final output is fully integrated, consistent, and ready for use.\";\n            } else if (consolidate_option === \"refactor\") {\n              queryInput.value = refactoringPrompt;\n            } else if (consolidate_option === \"critique\") {\n              queryInput.value =\n                \"Examine the attached project code files and provide a detailed critique of the code, including suggestions for improvements, potential issues, and best practices. Focus on code quality, readability, and maintainability.\";\n            } else if (consolidate_option === \"synopsis\") {\n              queryInput.value = synopsisPrompt;\n            }\n\n            consolidate_flag = false;\n          }\n        })\n        .catch((error) => {\n          console.error(\"Error fetching token:\", error);\n        });\n\n      copyPromptBtn.addEventListener(\"click\", async () => {\n        const currentQuery = queryInput.value;\n\n        // Fetch the latest file contents from the backend\n        let fileContents = \"\";\n        try {\n          const response = await fetch(\"/get_file_contents\");\n          if (response.ok) {\n            const data = await response.json();\n            fileContents = data.file_contents || \"\";\n          } else {\n            console.warn(\"Failed to fetch file contents from backend.\");\n          }\n        } catch (err) {\n          console.error(\"Error fetching file contents from backend:\", err);\n        }\n\n        const fullPrompt = fileContents\n          ? currentQuery + \" \\n  \\n \" + fileContents\n          : currentQuery;\n\n        if (fullPrompt.trim() === \"\") {\n          alert(\"Nothing to copy (input and file contents are empty).\");\n          return;\n        }\n\n        navigator.clipboard\n          .writeText(fullPrompt)\n          .then(() => {\n            const originalText = copyPromptBtn.innerText;\n            copyPromptBtn.innerText = \"Copied!\";\n            copyPromptBtn.disabled = true; // Briefly disable\n            console.log(\"Prompt copied to clipboard.\");\n            setTimeout(() => {\n              copyPromptBtn.innerText = originalText;\n              copyPromptBtn.disabled = false;\n            }, 2000); // Reset after 2 seconds\n          })\n          .catch((err) => {\n            console.error(\"Failed to copy prompt: \", err);\n            alert(\n              \"Failed to copy prompt to clipboard. See console for details.\"\n            );\n          });\n      });\n\n      clear_scr.addEventListener(\"click\", async () => {\n        queryInput.value = \"\";\n        autoResize();\n        preview.innerHTML = \"\";\n        markdownInput.value = \"\";\n      });\n\n      choose_model.addEventListener(\"click\", async () => {\n        let selectedModel;\n        selectedModel = dropdown_menu.value;\n        model_name = selectedModel;\n        try {\n          const response = await fetch(\"/select_model\", {\n            method: \"POST\",\n            headers: {\n              \"Content-Type\": \"application/json\",\n            },\n            body: JSON.stringify({\n              uniqueId: uniqueId,\n              selectedModel: selectedModel,\n            }),\n          });\n\n          if (response.ok) {\n            streaming_flag = ![\"o1-pro\"].includes(model_name);\n            markdownInput.value += `\\n\\n Unique Session ID: ${uniqueId} \\n`;\n            markdownInput.value += `\\n Model: ${model_name} \\n`;\n            markdownInput.value += `\\n Streaming: ${streaming_flag} \\n`;\n            renderMarkdown(true);\n            autoResize();\n          } else {\n            console.error(\"Failed to choose model\");\n          }\n        } catch (error) {\n          console.error(\"Error choosing model:\", error);\n        }\n      });\n\n      // Auto-resize Function\n      function autoResize() {\n        queryInput.style.height = \"auto\"; // Reset height\n        const scrollHeight = queryInput.scrollHeight;\n        const maxHeight = 350;\n        if (scrollHeight < maxHeight) {\n          queryInput.style.height = scrollHeight + \"px\";\n        } else {\n          queryInput.style.height = maxHeight + \"px\";\n        }\n      }\n\n      // Initial Resize\n      autoResize();\n\n      // Event Listener for Input\n      queryInput.addEventListener(\"input\", autoResize);\n\n      SaveBtn.addEventListener(\"click\", async () => {\n        queryInput.value = \"\";\n        autoResize();\n\n        try {\n          const response = await fetch(\"/choose_save_path\");\n          const data = await response.json();\n\n          if (response.ok && data.path) {\n            await saveConversation(data.path); // Pass the full path\n          } else if (data.error) {\n            // Handle cancellation or error from the dialog endpoint\n            console.log(\"Save cancelled or failed:\", data.error);\n          } else {\n            console.log(\"Save cancelled by user.\");\n          }\n        } catch (error) {\n          console.error(\"Error getting save path:\", error);\n          alert(\"An error occurred while trying to open the save dialog.\");\n        }\n      });\n\n      async function saveConversation(fullPath) {\n        try {\n          // Collect the markdown content from the editor\n          const markdownContent = markdownInput.value;\n\n          // Send POST request to the backend /save endpoint\n          const response = await fetch(\"/save\", {\n            method: \"POST\",\n            headers: {\n              \"Content-Type\": \"application/json\",\n            },\n            body: JSON.stringify({\n              uniqueId: uniqueId,\n              full_path: fullPath,\n              content: markdownContent,\n            }),\n          });\n\n          const result = await response.json();\n\n          if (response.ok) {\n            alert(`Conversation saved successfully to ${fullPath}!`);\n          } else {\n            alert(\n              `Failed to save conversation: ${result.error || \"Unknown error\"}`\n            );\n          }\n        } catch (error) {\n          console.error(\"Error saving conversation:\", error);\n          alert(\"An error occurred while saving the conversation.\");\n        }\n      }\n\n      closeBtn.addEventListener(\"click\", async () => {\n        queryInput.value = \"\";\n        autoResize();\n        currentFileContents = \"\";\n        preview.innerHTML = \"\";\n        markdownInput.value = \"\";\n\n        fetch(\"/shutdown\", {\n          method: \"POST\",\n          headers: {\n            \"Content-Type\": \"application/json\",\n          },\n          body: JSON.stringify({\n            timestamp: new Date().toISOString(),\n          }),\n        })\n          .then((response) => response.json())\n          .then((data) => {\n            if (data.message) {\n              alert(data.message);\n            } else if (data.error) {\n              alert(`Error: ${data.error}`);\n            }\n          })\n          .catch((error) => {\n            console.error(\"Error:\", error);\n          });\n      });\n\n      let renderTimeout;\n\n      function renderMarkdown(renderMath = false) {\n        clearTimeout(renderTimeout);\n\n        let markdownText = markdownInput.value;\n\n        marked.setOptions({\n          breaks: true,\n          highlight: function (code, lang) {\n            if (lang && hljs.getLanguage(lang)) {\n              return hljs.highlight(code, { language: lang }).value;\n            } else {\n              return hljs.highlightAuto(code).value;\n            }\n          },\n        });\n        let html = marked.parse(markdownText);\n        preview.innerHTML = html;\n        addCopyButtons();\n\n        if (renderMath) {\n          renderTimeout = setTimeout(() => {\n            renderMathJax();\n          }, 500); // 500ms delay\n        }\n      }\n\n      function addCopyButtons() {\n        const codeBlocks = preview.querySelectorAll(\"pre\");\n        codeBlocks.forEach((pre) => {\n          if (!pre.querySelector(\"button.copy-btn\")) {\n            const button = document.createElement(\"button\");\n            button.className = \"copy-btn\";\n            button.innerText = \"Copy\";\n            button.onclick = () => {\n              const code = pre.querySelector(\"code\").innerText;\n              navigator.clipboard\n                .writeText(code)\n                .then(() => {\n                  button.innerText = \"Copied!\";\n                  setTimeout(() => {\n                    button.innerText = \"Copy\";\n                  }, 2000);\n                })\n                .catch((err) => {\n                  console.error(\"Failed to copy: \", err);\n                });\n            };\n            pre.appendChild(button);\n          }\n        });\n      }\n\n      function renderMathJax() {\n        if (window.MathJax) {\n          MathJax.typesetPromise([preview]).catch(function (err) {\n            console.error(\"MathJax typeset failed: \" + err.message);\n          });\n        }\n      }\n\n      function appendConversationTurn(userQuery) {\n        markdownInput.value += `\\n\\n**Model:** ${model_name} \\n\\n**Assistant:** \\n\\n`;\n        renderMarkdown(true);\n      }\n\n      submitBtn.addEventListener(\"click\", async () => {\n        const query = queryInput.value.trim();\n        if (!query) return;\n\n        queryInput.value = \"\";\n        autoResize();\n        submitBtn.disabled = true;\n        appendConversationTurn(query);\n        currentFileContents = \"\";\n\n        let isMathRendered = false;\n\n        const url = `/stream?prompt=${encodeURIComponent(\n          query\n        )}&uniqueId=${encodeURIComponent(\n          uniqueId\n        )}&modelname=${encodeURIComponent(model_name)}`;\n\n        if (streaming_flag) {\n          try {\n            const response = await fetch(url);\n            if (!response.ok) {\n              throw new Error(`Network error: ${response.statusText}`);\n            }\n\n            const reader = response.body.getReader();\n            const decoder = new TextDecoder(\"utf-8\");\n            let buffer = \"\";\n            let done = false;\n\n            while (!done) {\n              const { value, done: doneReading } = await reader.read();\n              done = doneReading;\n              // Accumulate the stream text\n              buffer += decoder.decode(value, { stream: true });\n\n              // Split into lines (each line should be a complete JSON string)\n              let lines = buffer.split(\"\\n\");\n              // Keep any incomplete line in the buffer\n              buffer = lines.pop();\n\n              for (const line of lines) {\n                if (line.trim()) {\n                  try {\n                    // Parse the JSON object from the line\n                    const parsed = JSON.parse(line);\n                    let escapedContent;\n                    if (parsed.content) {\n                      if (math_render_flag) {\n                        // Escape backslashes: replace \"\\\" with \"\\\\\"\n                        escapedContent = parsed.content.replace(/\\\\/g, \"\\\\\\\\\");\n                      } else {\n                        escapedContent = parsed.content;\n                      }\n                      // Append only the escaped content to the Editor\n                      markdownInput.value += escapedContent;\n                    }\n                  } catch (err) {\n                    console.error(\"Error parsing JSON:\", err);\n                  }\n                }\n              }\n              // Scroll the Editor to the bottom for the latest content\n              markdownInput.scrollTop = markdownInput.scrollHeight;\n            }\n\n            // Once the stream is complete, update the Preview pane.\n            if (!isMathRendered) {\n              // Render the final markdown content to update the preview.\n              let sanitized = sanitizeText(markdownInput.value);\n              //  markdownInput.value = unescapeHtmlEntitiesInFencedCodeBlocks(sanitized);\n              markdownInput.value = sanitized;\n              renderMarkdown(true);\n              isMathRendered = true;\n            }\n            submitBtn.disabled = false;\n          } catch (error) {\n            console.error(\"Streaming error:\", error);\n            submitBtn.disabled = false;\n            alert(\n              \"An error occurred while streaming the response. Please try again.\"\n            );\n          }\n        } else {\n          try {\n            const response = await fetch(url);\n            if (!response.ok) {\n              throw new Error(`Network error: ${response.statusText}`);\n            }\n            const result = await response.json();\n            if (result.content) {\n              markdownInput.value += result.content;\n            } else {\n              alert(result.error || \"Unexpected error\");\n            }\n            // Scroll the Editor to the bottom for the latest content\n            markdownInput.scrollTop = markdownInput.scrollHeight;\n            if (!isMathRendered) {\n              // Render the final markdown content to update the preview.\n              let sanitized = sanitizeText(markdownInput.value);\n              //  markdownInput.value = unescapeHtmlEntitiesInFencedCodeBlocks(sanitized);\n              markdownInput.value = sanitized;\n              renderMarkdown(true);\n              isMathRendered = true;\n            }\n            submitBtn.disabled = false;\n          } catch (error) {\n            console.error(\"Non-streaming error:\", error);\n            submitBtn.disabled = false;\n            alert(\"An error occurred. Please try again.\");\n          }\n        }\n      });\n\n      resetBtn.addEventListener(\"click\", async () => {\n        try {\n          const response = await fetch(\"/reset\", {\n            method: \"POST\",\n            headers: {\n              \"Content-Type\": \"application/json\",\n            },\n            body: JSON.stringify({\n              uniqueId: uniqueId,\n            }),\n          });\n\n          if (response.ok) {\n            currentFileContents = \"\";\n            markdownInput.value = \"\";\n            preview.innerHTML = \"\";\n            queryInput.value = \"\";\n            markdownInput.value += `\\n Unique Session ID: ${uniqueId} \\n`;\n            markdownInput.value += `\\n Model: ${model_name} \\n`;\n            renderMarkdown();\n            autoResize();\n          } else {\n            console.error(\"Failed to reset conversation\");\n          }\n        } catch (error) {\n          console.error(\"Error resetting conversation:\", error);\n        }\n      });\n\n      queryInput.addEventListener(\"keydown\", (e) => {\n        if (e.key === \"Enter\") {\n          if (e.ctrlKey) {\n            // Ctrl + Enter: Submit the message\n            e.preventDefault(); // Prevent newline insertion\n            if (submitBtn && !submitBtn.disabled) {\n              submitBtn.click(); // Trigger send button if it's enabled\n            }\n          }\n          // Else: Allow newline insertion\n        }\n      });\n\n      // ———— file‐browser logic (adapted from index2.html) ————\n\n      let selectedFolder = null;\n\n      // recursively build tree\n      function createTree(node, container) {\n        if (Array.isArray(node)) {\n          node.forEach((n) => createTree(n, container));\n          return;\n        }\n        const wrapper = document.createElement(\"div\");\n        wrapper.className = \"node\";\n\n        if (node.type === \"directory\") {\n          // collapse/expand\n          const toggle = document.createElement(\"span\");\n          toggle.className = \"toggle-triangle\";\n          toggle.textContent = \"▼\";\n          wrapper.appendChild(toggle);\n\n          const label = document.createElement(\"span\");\n          label.className = \"directory-entry\";\n          label.textContent = node.name;\n          wrapper.appendChild(label);\n\n          container.appendChild(wrapper);\n\n          if (node.children && node.children.length) {\n            const childContainer = document.createElement(\"div\");\n            wrapper.appendChild(childContainer);\n            createTree(node.children, childContainer);\n\n            toggle.onclick = () => {\n              if (childContainer.style.display === \"none\") {\n                childContainer.style.display = \"\";\n                toggle.textContent = \"▼\";\n              } else {\n                childContainer.style.display = \"none\";\n                toggle.textContent = \"▶\";\n              }\n            };\n          } else {\n            toggle.style.visibility = \"hidden\";\n          }\n        } else if (node.type === \"file\") {\n          const link = document.createElement(\"a\");\n          link.href = \"#\";\n          link.className = \"file-link\";\n          link.textContent = node.name;\n\n          link.onclick = (e) => {\n            e.preventDefault();\n            if (!selectedFolder) {\n              alert(\"Open a folder first.\");\n              return;\n            }\n            // relative path keeps the URL short & avoids leaking absolute paths\n            const relPath = node.path.substring(selectedFolder.length + 1);\n            window.open(\n              `/load_chat?rel=${encodeURIComponent(relPath)}`,\n              \"_blank\"\n            );\n          };\n\n          wrapper.appendChild(link);\n          container.appendChild(wrapper);\n        }\n      }\n\n      function loadDirectoryTree(tree) {\n        const div = document.getElementById(\"directory-tree\");\n        div.innerHTML = \"\";\n        createTree(tree, div);\n      }\n\n      // “Open Directory” exactly as index2.html does:\n      document.getElementById(\"open-folder\").onclick = async () => {\n        try {\n          const resp = await fetch(\"/open_folder_chat\", { method: \"POST\" });\n          const data = await resp.json();\n          if (data.error) {\n            alert(data.error);\n          } else {\n            selectedFolder = data.folder;\n            loadDirectoryTree(data.structure);\n          }\n        } catch (err) {\n          console.error(err);\n          alert(\"Failed to open folder\");\n        }\n      };\n\n      // “Refresh” exactly as index2.html does:\n      document.getElementById(\"refresh\").onclick = async () => {\n        if (!selectedFolder) {\n          alert(\"No folder selected\");\n          return;\n        }\n        try {\n          const resp = await fetch(\"/refresh_chat\");\n          const data = await resp.json();\n          if (data.error) {\n            alert(data.error);\n          } else {\n            loadDirectoryTree(data.structure);\n          }\n        } catch (err) {\n          console.error(err);\n          alert(\"Failed to refresh\");\n        }\n      };\n    </script>\n  </body>\n</html>\n\n\n---\n\nFILE: app.py\n\nimport sys\nimport os\nimport base64\nimport socket \nfrom flask import copy_current_request_context, json, Flask, render_template, request, session, jsonify, Response, stream_with_context, send_from_directory\nfrom openai import OpenAI\nimport anthropic\nfrom google import genai\nfrom google.genai import types\nfrom dotenv import load_dotenv\nfrom flask_session import Session\nfrom waitress import serve \nimport logging\nfrom typing import Generator, Dict\nfrom datetime import datetime, UTC\nimport uuid\nimport tkinter as tk\nfrom tkinter import filedialog\n\n\n# Initial system message for chatbot\nSYSTEM_MESSAGE = \"You are an expert in coding. \" \\\n\"Write code that is clean, efficient, modular, and well-documented. \" \\\n\"Ensure all code block are encased with triple backticks.\"\n\n# Transient global variables\nunique_id_1 = None\nmodel_name_1 = None\nstreaming_flag_1 = False\ncounter_1 = 0\n\n# Global variable to store markdown content\nloaded_markdown_content: str = \"\"\n\n# Global variable to store the selected folder path for chat\nselected_folder_chat = None \n\n# Option corresponding to consolidation of conversation\nselected_option_consolidate = \"fully_updated\"\n\n# Flag to indicate if conversation should be consolidated\nconsolidate_conversation_flag = False\n\n# Global variable to store the loaded conversation messages\nloaded_conversation_messages = []\n\n# Contents of selected files\nglobal_file_contents = \"\"\n\n# Initial model name\nmodel_name = \"chatgpt-4o-latest\"\n\n# Global variable to store the selected folder path.\nselected_folder = None\n\n# Flag to indicate if math rendering is enabled\nmath_render_flag = False\n\n# Flag to indicate if Claude is thinking\nclaude_thinking_flag = False\n\n# Flag to indicate if a new file is being loaded\nloading_new_file = False\n\n# Get command line arguments\narguments = sys.argv[1:]  # sys.argv[0] is the script name itself.\n\n# Load environment variables from .env file\nload_dotenv()\n\n# Initialize Flask app\napp = Flask(__name__)\n#— enable debug + auto‑reload of code & templates —\napp.config['DEBUG'] = True\napp.config['TEMPLATES_AUTO_RELOAD'] = True\n#— disable caching of static files so changed JS/CSS/html always reloads —\napp.config['SEND_FILE_MAX_AGE_DEFAULT'] = 0\n\n# Configure with strong secret key\napp.secret_key = os.getenv('FLASK_SECRET_KEY') or os.urandom(24)\n\n# Configure server-side session\napp.config['SESSION_TYPE'] = 'filesystem'\napp.config['SESSION_PERMANENT'] = False\nSession(app)\nmessage_accumulate = []\n\n# Configure logging with proper error handling\nlogging.basicConfig(\n    level=logging.DEBUG,\n    filename='app.log',\n    format='%(asctime)s %(levelname)s %(name)s %(threadName)s : %(message)s'\n)\nlogger = logging.getLogger(__name__)\n\n# Define directories\nCONVERSATIONS_DIR = os.path.join(os.getcwd(), 'conversations')\n\n# Ensure directories exist\nos.makedirs(CONVERSATIONS_DIR, exist_ok=True)\n\n# Initialize OpenAI client with error handling\ntry:\n    api_key = os.getenv(\"OPENAI_API_KEY\")\n    if not api_key:\n        raise ValueError(\"OPENAI_API_KEY environment variable is not set\")\n    client = OpenAI(api_key=api_key)\nexcept Exception as e:\n    if \"openai\" in arguments or \"chatgpt\" in arguments:\n        logger.critical(f\"Failed to initialize OpenAI client: {e}\")\n        raise\n    else:\n        logger.info(\"OpenAI client initialization skipped.\")\n        client = None  # Explicitly set client to None if initialization fails or skipped\n\n# Initialize Anthropic client with error handling\ntry:\n    anthropic_api_key = os.getenv(\"ANTHROPIC_API_KEY\")\n    if not anthropic_api_key:\n        raise ValueError(\"ANTHROPIC_API_KEY environment variable is not set\")\n    anthropic_client = anthropic.Anthropic(api_key=anthropic_api_key)\nexcept Exception as e:\n    if \"anthropic\" in arguments or \"claude\" in arguments:\n        logger.critical(f\"Failed to initialize Anthropic client: {e}\")\n        raise\n    else:\n        logger.info(\"Anthropic client initialization skipped.\")\n        anthropic_client = None  # Explicitly set client to None if initialization fails or skipped\n\n# Initialize Gemini client with error handling\ntry:\n    gemini_api_key = os.getenv(\"GEMINI_API_KEY\")\n    if not gemini_api_key:\n        raise ValueError(\"GEMINI_API_KEY environment variable is not set\")\n    gemini_client = genai.Client(api_key=gemini_api_key)\nexcept Exception as e:\n    if \"gemini\" in arguments or \"google\" in arguments:\n        logger.critical(f\"Failed to initialize Gemini client: {e}\")\n        raise\n    else:\n        logger.info(\"Gemini client initialization skipped.\")\n        gemini_client = None  # Explicitly set client to None if initialization fails or skipped\n\n\ndef save_system_messages(timestamp, filename):\n    conversation_data = {\n        'messages': session['messages'],\n        'last_updated': timestamp\n        }\n    filename = filename.replace(\":\",\"\")\n    conversation_path = os.path.join(CONVERSATIONS_DIR, f\"{filename}.json\")\n    try:\n        with open(conversation_path, 'w', encoding='utf-8') as f:\n            json.dump(conversation_data, f, indent=2, ensure_ascii=False)\n    except Exception as e:\n        print(f\"Error saving user prompts: {e}\")\n        return jsonify({'error': 'Failed to save user prompts.'}), 500\n    \n    \n    conversation_path = os.path.join(CONVERSATIONS_DIR, f\"{filename+'all'}.json\")\n    try:\n        with open(conversation_path, 'w', encoding='utf-8') as f:\n            json.dump(message_accumulate, f, indent=2, ensure_ascii=False)\n    except Exception as e:\n        print(f\"Error saving user prompts: {e}\")\n        return jsonify({'error': 'Failed to save user prompts.'}), 500\n    \n    return jsonify({'message': 'User prompts saved successfully.'}), 200\n\n\n\ndef determine_counter(unique_id):\n    global message_accumulate\n    counter = 0\n    for messages in message_accumulate:\n        if messages[0] == unique_id:\n            if (int(messages[1]) > counter):\n                counter = int(messages[1])\n    return counter\n\n\ndef get_conversation_history(unique_id, counter):\n    global message_accumulate \n    conversation_history = []\n    for messages in message_accumulate:\n            if (messages[0] == unique_id) and (messages[1] == counter):\n                conversation_history.append(messages[2])\n\n    return conversation_history\n\n\ndef get_markdown_for_conversation(path):\n    \"\"\"\n    Given a .json or .md path, return the corresponding .md file's contents (if exists).\n    \"\"\"\n    stem, ext = os.path.splitext(path)\n    if ext.lower() == \".json\" and stem.endswith(\"messages\"):\n        md_path = stem[:-len(\"messages\")] + \".md\"\n    elif ext.lower() in (\".md\", \".txt\"):\n        md_path = stem + \".md\"\n    else:\n        return None, None\n    if os.path.isfile(md_path):\n        try:\n            with open(md_path, \"r\", encoding=\"utf-8\") as f:\n                return md_path, f.read()\n        except Exception as e:\n            return md_path, f\"Error reading markdown file: {e}\"\n    return md_path, \"\"\n\n\n@app.route('/')\ndef index() -> str:\n    global message_accumulate\n    global consolidate_conversation_flag\n    if 'messages' not in session:\n        session['messages'] = []\n    session['messages'].append({\n                \"role\": \"system\",\n                \"content\": SYSTEM_MESSAGE\n                })\n    session.modified = True\n    return render_template('index.html')\n\n\n@app.route('/get-token', methods=['GET'])\ndef get_token():\n    global message_accumulate\n    global model_name\n    global loaded_conversation_messages\n    global loading_new_file\n    global consolidate_conversation_flag\n    global selected_option_consolidate\n    global unique_id_1\n    global model_name_1\n    global loaded_markdown_content\n    token = str(uuid.uuid4())\n    if loading_new_file:\n        for dict_msg in loaded_conversation_messages:\n            message_accumulate.append([token, 1, dict_msg])\n    elif consolidate_conversation_flag:\n        counter_1 = determine_counter(unique_id_1)\n        dd = []\n        for dict_msg in message_accumulate:\n            if (dict_msg[0] == unique_id_1) and (dict_msg[1] == counter_1):\n                dd.append(dict_msg[2])\n        for msg in dd:\n            message_accumulate.append([token, 1, msg])\n    else:\n        message_accumulate.append([token, 1, {\"role\": \"system\", \"content\": SYSTEM_MESSAGE}])\n    loaded_conversation_messages = []\n    loading_new_file = False\n    c = consolidate_conversation_flag\n    consolidate_conversation_flag = False\n    markdown_to_send = loaded_markdown_content or \"\"\n    loaded_markdown_content = \"\"   \n    return jsonify({'token': token, \n                    'model_name': model_name,\n                    'consolidate_conversation_flag': c,  \n                    'consolidate_option': selected_option_consolidate,\n                    'markdown_content': markdown_to_send,\n                    })  # Send token as JSON response\n\n\n@app.route('/update-claude-thinking', methods=['POST'])\ndef update_claude_thought_state():\n    global claude_thinking_flag\n    data = request.get_json()\n    claude_thinking_flag = data.get('claude_thinking_flag')\n    return jsonify({'status': 'updated', 'value': claude_thinking_flag}), 200\n\n@app.route('/update-math-render', methods=['POST'])\ndef update_math_render_state():\n    global math_render_flag\n    data = request.get_json()\n    math_render_flag = data.get('math_render_flag')\n    return jsonify({'status': 'updated', 'value': math_render_flag}), 200\n\n# Load Conversation Endpoint\n@app.route('/load', methods=['GET'])\ndef load_conversation():\n    global loaded_conversation_messages\n    global loading_new_file\n    global loaded_markdown_content\n\n    # 1) Ask the user to pick any file\n    selected = choose_file()\n    if not selected:\n        return jsonify({'error': 'No file selected.'}), 400\n\n    _, loaded_markdown_content = get_markdown_for_conversation(selected)\n    # 2) Compute your “conversation key” exactly as the tree‑view does\n    base = os.path.basename(selected)   # e.g. \"123.md\" or \"foo.messages.json\"\n    stem, ext = os.path.splitext(base)  # (\"123\", \".md\") or (\"foo.messages\", \".json\")\n    if ext.lower() == '.json' and stem.lower().endswith('messages'):\n        conv = stem[:-len('messages')]  # strip off trailing \"messages\"\n    else:\n        conv = stem                     # otherwise just the name before the last “.”\n    conv = conv.rstrip('.')             # guard against an accidental trailing dot\n\n    # 3) Build the full path in the same folder the file lived in\n    folder = os.path.dirname(selected)  # e.g. \"/home/bin\"\n    target = os.path.join(folder, f'{conv}messages.json')\n    \n    # 4) Load it\n    try:\n        with open(target, 'r', encoding='utf-8') as f:\n            loaded_conversation_messages = json.load(f)\n    except FileNotFoundError:\n        return jsonify({'error': f'Conversation not found at {target}'}), 404\n    except json.JSONDecodeError:\n        return jsonify({'error': 'Error decoding JSON.'}), 500\n    except Exception as e:\n        logger.error(f\"Error loading {target}: {e}\", exc_info=True)\n        return jsonify({'error': 'Failed to load conversation.'}), 500\n\n    loading_new_file = True\n    return render_template('index.html')\n\n\n# Consolidate Conversation Endpoint\n@app.route('/consolidate', methods=['GET'])\ndef consolidate_conversation():\n    global consolidate_conversation_flag\n    global unique_id_1\n    global model_name_1\n    global streaming_flag_1\n    global selected_option_consolidate\n    consolidate_conversation_flag = True\n    unique_id_1 = request.args.get(\"uniqueId\")\n    model_name_1 = request.args.get(\"model_name\")\n    streaming_flag_1 = request.args.get(\"streaming_flag\")\n    selected_option_consolidate = request.args.get(\"option\", \"fully_updated\")\n    return render_template('index.html')\n\n# Endpoint to trigger the native \"Save As\" dialog\n@app.route('/choose_save_path', methods=['GET'])\ndef choose_save_path():\n    \"\"\"\n    Opens a native 'Save As' dialog using Tkinter on the server side.\n    Returns the chosen file path to the client.\n    \"\"\"\n    root = None\n    try:\n        root = tk.Tk()\n        root.withdraw()  # Hide the main Tkinter window\n        # Pop up the \"Save As\" dialog\n        file_path = filedialog.asksaveasfilename(\n            title=\"Save Conversation As...\",\n            defaultextension=\".md\", # Default extension\n            filetypes=[(\"Markdown files\", \"*.md\"), # File type filter\n                       (\"Text files\", \"*.txt\"),\n                       (\"JSON files\", \"*.json\")]\n        )\n        root.destroy() # Clean up the Tkinter root window\n\n        if file_path:\n            # User selected a path\n            return jsonify({'path': file_path})\n        else:\n            # User cancelled the dialog\n            return jsonify({'error': 'No path selected'}), 400\n    except Exception as e:\n        logger.error(f\"Error opening save dialog: {e}\", exc_info=True)\n        if root:\n            try:\n                root.destroy()\n            except: # nosec # Ignore errors during cleanup\n                pass\n        return jsonify({'error': 'Failed to open save dialog.'}), 500\n    \n\n# Save Conversation Endpoint\n@app.route('/save', methods=['POST'])\ndef save_conversation():\n    global message_accumulate\n    data = request.get_json()\n    full_path = data.get('full_path')\n    content = data.get('content')\n    unique_id = data.get('uniqueId')\n    counter = determine_counter(unique_id)\n\n    # Validate the received full path\n    if not full_path or not content:\n        return jsonify({'error': 'Full path and content are required.'}), 400\n\n    # Determine the directory and base filename from the full path\n    try:\n        save_directory = os.path.dirname(full_path)\n        base_filename = os.path.basename(full_path)\n        filename_stem, _ = os.path.splitext(base_filename) # Get name without extension\n\n        # Ensure the target directory exists (optional, but good practice)\n        os.makedirs(save_directory, exist_ok=True)\n\n        # Construct the paths using the chosen directory and derived names\n        markdown_path = os.path.join(save_directory, f\"{filename_stem}.md\")\n        json_path = os.path.join(save_directory, f\"{filename_stem}messages.json\")\n\n    except Exception as e:\n        logger.error(f\"Error processing save path '{full_path}': {e}\", exc_info=True)\n        return jsonify({'error': f'Invalid save path provided: {e}'}), 400\n\n    \n    conversation_messages_only = get_conversation_history(unique_id, counter)\n\n    try:\n        with open(json_path, 'w', encoding='utf-8') as f:\n            json.dump(conversation_messages_only, f, indent=2, ensure_ascii=False)\n    except Exception as e:\n        print(f\"Error saving conversation: {e}\")\n        return jsonify({'error': 'Failed to save conversation.'}), 500\n\n    md_lines = []\n    # Header with file name\n    md_lines.append(f\"**FILE:** {markdown_path}\\n\\n\")\n\n    # Iterate each message and format\n    for idx, msg in enumerate(conversation_messages_only, start=1):\n        md_lines.append(\"---\\n\\n\")\n        md_lines.append(f\"**Dictionary Item {idx:02d}**\\n\\n\")\n        md_lines.append(f\"**Role:** {msg.get('role','')}\\n\\n\")\n        md_lines.append(\"**Content:**\\n\\n\")\n        md_lines.append(f\"{msg.get('content','')}\\n\\n\")\n        md_lines.append(\"---\\n\")\n\n    try:\n        with open(markdown_path, 'w', encoding='utf-8') as f:\n            f.writelines(md_lines)\n    except Exception as e:\n        print(f\"Error saving markdown: {e}\")\n        return jsonify({'error': 'Failed to save markdown file.'}), 500\n    \n    return jsonify({'message': 'Conversation and markdown saved successfully.'}), 200\n\n\n\n@app.route('/shutdown', methods=['POST'])\ndef shutdown():\n    global message_accumulate\n    data = request.get_json()\n    timestamp = data.get('timestamp', datetime.now(UTC))\n    filename = str(timestamp).replace(\" \", \"\")\n    _ = save_system_messages(timestamp, filename)\n    message_accumulate = []\n    session['messages'] = []\n    session.modified = True\n    \n    logger.info(\"Shutting down the server...\")\n    os._exit(0)  # Forcefully terminate the process\n    return jsonify({\"message\": \"Server is shutting down...\"}), 200\n\n\n\n@app.route('/stream', methods=['GET'])\ndef stream():\n    global global_file_contents\n    global message_accumulate\n    global model_name\n    global claude_thinking_flag\n    try:\n        # Get prompt from URL parameters\n        user_input = request.args.get('prompt')\n        unique_id = request.args.get('uniqueId')\n        model_name = request.args.get('modelname')\n\n        if (not user_input) or (len(user_input.strip()) == 0):\n            return jsonify({'error': 'No prompt provided'}), 400\n\n        if (not unique_id) or (len(unique_id.strip()) == 0):\n            return jsonify({'error': 'No Unique Session ID provided'}), 400\n        \n        if (not model_name) or (len(model_name.strip()) == 0):\n            return jsonify({'error': 'Invalid Model Name provided'}), 400\n        \n        \n        counter = determine_counter(unique_id)\n\n        # Append user message to session\n        user_input = user_input + \"\\n\" + global_file_contents\n        global_file_contents = \"\"\n        message_accumulate.append([unique_id, counter, {\"role\": \"user\", \"content\": user_input}])\n        conversation_history = get_conversation_history(unique_id, counter)\n\n        session['messages'].append({\"role\": \"user\", \"content\": user_input})\n        session.modified = True  # Ensure session is saved\n\n        @copy_current_request_context\n        def generate_response(unique_id: str, counter: int, conversation_history: list[Dict]) -> Generator[str, None, None]:\n            global message_accumulate\n            global model_name\n            global claude_thinking_flag\n            collected_chunks = ['\\n']\n\n            if \"4o\" in model_name:\n\n                # Construct full prompt with system message\n                full_prompt = []\n                full_prompt.append(\n                    {\"role\": \"system\", \n                     \"content\": [{\"type\": \"text\", \"text\": SYSTEM_MESSAGE}]\n                     }\n                )\n                for messagedict in conversation_history:\n                    if messagedict['role'] == 'user':\n                        full_prompt.append(\n                            {\"role\": \"user\", \n                             \"content\": [{\"type\": \"text\", \"text\": messagedict['content']}]\n                             }\n                        )\n                    elif messagedict['role'] == 'assistant':\n                        full_prompt.append(\n                            {\"role\": \"assistant\", \n                             \"content\": [{\"type\": \"text\", \"text\": messagedict['content']}]\n                             }\n                        )\n\n                try:\n                    # Call OpenAI API with conversation history and streaming enabled\n                    response = client.chat.completions.create(\n                        model=model_name,\n                        messages=full_prompt,\n                        response_format={\"type\": \"text\"},\n                        temperature=0,\n                        max_completion_tokens=8192,\n                        top_p=1,\n                        frequency_penalty=0,\n                        presence_penalty=0,\n                        store=False,\n                        stream=True,\n                    )\n             \n                    for chunk in response:\n                        if chunk.choices[0].delta.content is not None:\n                            chunk_message = chunk.choices[0].delta.content\n                            collected_chunks.append(chunk_message)\n                            ##  chunk_message = chunk_message.replace(\"\\\\\", \"\\\\\\\\\")\n                            yield json.dumps({'content': chunk_message}) + \"\\n\"\n\n                    # After streaming, append the full bot message to the session\n                    bot_message = ''.join(collected_chunks)\n                    message_accumulate.append([unique_id, counter, {\"role\": \"assistant\", \"content\": bot_message}])\n\n                    # Emit 'done' event to signal completion\n                    yield json.dumps({'message': 'Stream complete'}) + \"\\n\"\n\n        \n                except Exception as e:\n                    logger.error(f\"Error in generate_response: {e}\", exc_info=True)\n                    yield json.dumps({'error': 'An error occurred while processing your request. Please try again.'}) + \"\\n\"\n\n\n            elif \"gpt-4.1\" in model_name:\n\n                # Construct full prompt with system message\n                full_prompt = []\n                full_prompt.append(\n                    {\"role\": \"system\", \n                     \"content\": [{\"type\": \"input_text\", \"text\": \"You are a helpful assistant. You provide comprehensive, insightful, forward-thinking responses to user queries.\"}]\n                     }\n                )\n                for messagedict in conversation_history:\n                    if messagedict['role'] == 'user':\n                        full_prompt.append(\n                            {\"role\": \"user\", \n                             \"content\": [{\"type\": \"input_text\", \"text\": messagedict['content']}]\n                             }\n                        )\n                    elif messagedict['role'] == 'assistant':\n                        full_prompt.append(\n                            {\"role\": \"assistant\", \n                             \"content\": [{\"type\": \"output_text\", \"text\": messagedict['content']}]\n                             }\n                        )\n\n                try:\n                    # Call OpenAI API with conversation history\n                    response = client.responses.create(\n                        model=model_name,\n                        input=full_prompt,\n                        temperature=0,\n                        max_output_tokens=16384,\n                        top_p=1,\n                        text={\"format\": {\"type\": \"text\"}},\n                        reasoning={},\n                        tools=[],\n                        store=True,\n                    )\n                    bot_message = response.output_text\n                    yield json.dumps({'content': bot_message}) + \"\\n\"\n                    message_accumulate.append([unique_id, counter, {\"role\": \"assistant\", \"content\": bot_message}])\n        \n                except Exception as e:\n                    logger.error(f\"Error in generate_response: {e}\", exc_info=True)\n                    yield json.dumps({'error': 'An error occurred while processing your request. Please try again.'}) + \"\\n\"\n\n\n\n            elif \"o4-mini\" in model_name or \"o3\" in model_name: \n\n                # Construct full prompt with system message\n                full_prompt = []\n                full_prompt.append(\n                    {\"role\": \"developer\", \n                     \"content\": [{\"type\": \"input_text\", \"text\": \"You are a helpful assistant. You provide comprehensive, insightful, forward-thinking responses to user queries.\"}]\n                     }\n                )\n                for messagedict in conversation_history:\n                    if messagedict['role'] == 'user':\n                        full_prompt.append(\n                            {\"role\": \"user\", \n                             \"content\": [{\"type\": \"input_text\", \"text\": messagedict['content']}]\n                             }\n                        )\n                    elif messagedict['role'] == 'assistant':\n                        full_prompt.append(\n                            {\"role\": \"assistant\", \n                             \"content\": [{\"type\": \"output_text\", \"text\": messagedict['content']}]\n                             }\n                        )\n\n                try:\n                    # Call OpenAI API with conversation history\n                    response = client.responses.create(\n                        model=model_name,\n                        input=full_prompt,\n                        text={\"format\": {\"type\": \"text\"}},\n                        reasoning={\"effort\": \"high\"},\n                        tools=[],\n                        store=True,\n                    )\n                    bot_message = response.output_text\n                    yield json.dumps({'content': bot_message}) + \"\\n\"\n                    message_accumulate.append([unique_id, counter, {\"role\": \"assistant\", \"content\": bot_message}])\n        \n                except Exception as e:\n                    logger.error(f\"Error in generate_response: {e}\", exc_info=True)\n                    yield json.dumps({'error': 'An error occurred while processing your request. Please try again.'}) + \"\\n\"\n\n\n\n\n\n            \n            elif \"o1-pro\" in model_name:\n\n                # Construct full prompt with system message\n                full_prompt = []\n                full_prompt.append(\n                    {\"role\": \"developer\", \n                     \"content\": [{\"type\": \"input_text\", \"text\": \"You are a helpful assistant. You provide comprehensive, insightful, forward-thinking responses to user queries.\"}]\n                     }\n                )\n                for messagedict in conversation_history:\n                    if messagedict['role'] == 'user':\n                        full_prompt.append(\n                            {\"role\": \"user\", \n                             \"content\": [{\"type\": \"input_text\", \"text\": messagedict['content']}]\n                             }\n                        )\n                    elif messagedict['role'] == 'assistant':\n                        full_prompt.append(\n                            {\"role\": \"assistant\", \n                             \"content\": [{\"type\": \"output_text\", \"text\": messagedict['content']}]\n                             }\n                        )\n\n                try:\n                    # Call OpenAI API with conversation history\n                    response = client.responses.create(\n                        model=model_name,\n                        input=full_prompt,\n                        text={\"format\": {\"type\": \"text\"}},\n                        reasoning={\"effort\": \"high\"},\n                        tools=[],\n                        store=False,\n                    )\n                    bot_message = response.output_text\n                    yield json.dumps({'content': bot_message}) + \"\\n\"\n                    message_accumulate.append([unique_id, counter, {\"role\": \"assistant\", \"content\": bot_message}])\n        \n                except Exception as e:\n                    logger.error(f\"Error in generate_response: {e}\", exc_info=True)\n                    yield json.dumps({'error': 'An error occurred while processing your request. Please try again.'}) + \"\\n\"\n\n\n            elif (\"o3-mini\" in model_name):\n                \n                # Construct full prompt with system message\n                full_prompt = []\n                full_prompt.append(\n                    {\"role\": \"developer\", \n                     \"content\": [{\"type\": \"text\", \"text\": SYSTEM_MESSAGE}]\n                     }\n                )\n                for messagedict in conversation_history:\n                    if messagedict['role'] == 'user':\n                        full_prompt.append(\n                            {\"role\": \"user\", \n                             \"content\": [{\"type\": \"text\", \"text\": messagedict['content']}]\n                             }\n                        )\n                    elif messagedict['role'] == 'assistant':\n                        full_prompt.append(\n                            {\"role\": \"assistant\", \n                             \"content\": [{\"type\": \"text\", \"text\": messagedict['content']}]\n                             }\n                        )\n\n                try:\n                    # Call OpenAI API with conversation history and streaming enabled\n                    response = client.chat.completions.create(\n                        model=model_name,\n                        messages=full_prompt,\n                        response_format={\"type\": \"text\"},\n                        reasoning_effort=\"high\",\n                        store=False,\n                        stream=True,\n                    )\n             \n                    for chunk in response:\n                        if chunk.choices[0].delta.content is not None:\n                            chunk_message = chunk.choices[0].delta.content\n                            collected_chunks.append(chunk_message)\n                            ##  chunk_message = chunk_message.replace(\"\\\\\", \"\\\\\\\\\")\n                            yield json.dumps({'content': chunk_message}) + \"\\n\"\n\n                    # After streaming, append the full bot message to the session\n                    bot_message = ''.join(collected_chunks)\n                    message_accumulate.append([unique_id, counter, {\"role\": \"assistant\", \"content\": bot_message}])\n\n                    # Emit 'done' event to signal completion\n                    yield json.dumps({'message': 'Stream complete'}) + \"\\n\"\n\n                except Exception as e:\n                    logger.error(f\"Error in generate_response: {e}\", exc_info=True)\n                    yield json.dumps({'error': 'An error occurred while processing your request. Please try again.'}) + \"\\n\"\n\n\n            elif \"claude\" in model_name:\n\n                # Construct full prompt with system message\n                full_prompt = []\n                for messagedict in conversation_history:\n                    if messagedict['role'] == 'user':\n                        full_prompt.append(\n                            {\"role\": \"user\", \n                             \"content\": [{\"type\": \"text\", \"text\": messagedict['content']}]\n                             }\n                        )\n                    elif messagedict['role'] == 'assistant':\n                        full_prompt.append(\n                            {\"role\": \"assistant\", \n                             \"content\": [{\"type\": \"text\", \"text\": messagedict['content']}]\n                             }\n                        )\n\n                try:\n                    if (claude_thinking_flag == True):\n                        with anthropic_client.messages.stream(\n                            model=model_name,\n                            max_tokens=32000,\n                            system = SYSTEM_MESSAGE,\n                            messages=full_prompt,\n                            thinking={\"type\": \"enabled\", \"budget_tokens\": 16000},\n                        ) as stream:\n                            for chunk_text in stream.text_stream:\n                                if chunk_text is not None:\n                                    chunk_message = chunk_text\n                                    collected_chunks.append(chunk_message)\n                                    yield json.dumps({'content': chunk_message}) + \"\\n\"\n                    else: \n\n                        with anthropic_client.messages.stream(\n                            model=model_name,\n                            max_tokens=32000,\n                            system = SYSTEM_MESSAGE,\n                            messages=full_prompt,\n                        ) as stream:\n                            for chunk_text in stream.text_stream:\n                                if chunk_text is not None:\n                                    chunk_message = chunk_text\n                                    collected_chunks.append(chunk_message)\n                                    yield json.dumps({'content': chunk_message}) + \"\\n\"\n\n                    # After streaming, append the full bot message to the session\n                    bot_message = ''.join(collected_chunks)\n                    message_accumulate.append([unique_id, counter, {\"role\": \"assistant\", \"content\": bot_message}])\n\n                    # Emit 'done' event to signal completion\n                    yield json.dumps({'message': 'Stream complete'}) + \"\\n\"\n                \n                except Exception as e:\n                    logger.error(f\"Error in generate_response: {e}\", exc_info=True)\n                    yield json.dumps({'error': 'An error occurred while processing your request. Please try again.'}) + \"\\n\"\n\n            \n            elif \"gemini\" in model_name:\n                # Construct full prompt with system message\n                full_prompt = []\n                for messagedict in conversation_history:\n                    if messagedict['role'] == 'user':\n                        full_prompt.append(\n                            types.Content(\n                                role=\"user\",\n                                parts=[\n                                    types.Part.from_text(text=messagedict['content']),\n                                ],\n                            ),\n                        )\n                    elif messagedict['role'] == 'assistant':\n                        full_prompt.append(\n                            types.Content(\n                                role=\"model\",\n                                parts=[\n                                    types.Part.from_text(text=messagedict['content']),\n                                ],\n                            ),\n                        )\n\n                generate_content_config = types.GenerateContentConfig(\n                    temperature=0,\n                    tools = [types.Tool(google_search=types.GoogleSearch())],\n                    safety_settings=[\n                        types.SafetySetting(\n                            category=\"HARM_CATEGORY_CIVIC_INTEGRITY\",\n                            threshold=\"OFF\",  \n                        ),\n                    ],\n                    response_mime_type=\"text/plain\",\n                    system_instruction=[\n                        types.Part.from_text(text=SYSTEM_MESSAGE),\n                    ],\n                )\n\n                try:\n                    # Call Gemini API with conversation history and streaming enabled\n                    stream = gemini_client.models.generate_content_stream(\n                        model=model_name,\n                        contents=full_prompt,\n                        config=generate_content_config,\n                    )\n             \n                    for chunk in stream:\n                        if hasattr(chunk, 'text') and chunk.text:\n                            chunk_message = chunk.text\n                            collected_chunks.append(chunk_message)\n                            ##  chunk_message = chunk_message.replace(\"\\\\\", \"\\\\\\\\\")\n                            yield json.dumps({'content': chunk_message}) + \"\\n\"\n\n                    # After streaming, append the full bot message to the session\n                    bot_message = ''.join(collected_chunks)\n                    message_accumulate.append([unique_id, counter, {\"role\": \"assistant\", \"content\": bot_message}])\n\n                    # Emit 'done' event to signal completion\n                    yield json.dumps({'message': 'Stream complete'}) + \"\\n\"\n\n        \n                except Exception as e:\n                    logger.error(f\"Error in generate_response: {e}\", exc_info=True)\n                    yield json.dumps({'error': 'An error occurred while processing your request. Please try again.'}) + \"\\n\"\n\n            \n            \n            else:\n                logger.error(\"Invalid Model\")\n                return jsonify({'error': \"Invalid Model\"}), 500\n\n        return Response(\n            stream_with_context(generate_response(unique_id, counter, conversation_history)),\n            mimetype='text/event-stream',\n            headers={'Cache-Control': 'no-cache'}\n        )\n\n\n    except Exception as e:\n        logger.error(f\"Error in stream route: {e}\", exc_info=True)\n        return jsonify({'error': str(e)}), 500\n\n\n\n@app.route('/reset', methods=['POST'])\ndef reset() -> Dict[str, str]:\n    global message_accumulate\n    data = request.get_json()\n    unique_id = data.get('uniqueId')\n    counter = determine_counter(unique_id) + 1\n    message_accumulate.append([unique_id, counter, {\"role\": \"system\", \"content\": SYSTEM_MESSAGE}])  \n    return jsonify({'status': 'Conversation reset successfully'})\n\n\n@app.route('/select_model', methods=['POST'])\ndef select_model() -> Dict[str, str]:\n    global message_accumulate\n    global model_name\n    data = request.get_json()\n    unique_id = data.get('uniqueId')\n    model_name = data.get('selectedModel')\n    return jsonify({'status': 'Model reset successfully'})\n\n\n@app.route('/get_file_contents', methods=['GET'])\ndef get_file_contents():\n    global global_file_contents\n    return jsonify({\"file_contents\": global_file_contents})\n\n\n\n#####################\n# File Contents Viewer\n#####################\n\n@app.route('/filecontents')\ndef filecontents():\n    return render_template('index2.html')\n\ndef choose_folder():\n    # Use Tkinter to open a native folder selection dialog.\n    root = tk.Tk()\n    root.withdraw()  # Hide the root window.\n    folder_path = filedialog.askdirectory(title=\"Select Folder\")\n    root.destroy()\n    return folder_path\n\n\n\ndef choose_file():\n    \"\"\"Pop up a standard file‑open dialog and return the chosen path (or '' if cancelled).\"\"\"\n    root = tk.Tk()\n    root.withdraw()\n    path = filedialog.askopenfilename(\n        title=\"Select a conversation base file\",\n        filetypes=[(\"All files\",\"*.*\")]\n    )\n    root.destroy()\n    return path\n\n\ndef get_directory_structure(rootdir):\n    \"\"\"\n    Recursively builds a directory structure.\n    Only includes files with specific extensions.\n    \"\"\"\n    allowed_extensions = {'.text', '.txt', '.md', '.html', '.htm', '.php', '.js', '.css', '.py', '.json', '.ipynb'}\n    structure = []\n\n    try:\n        for item in os.listdir(rootdir):\n            full_path = os.path.join(rootdir, item)\n            if os.path.isdir(full_path):\n                children = get_directory_structure(full_path)\n                if children:  # Only include directories that have valid children\n                    structure.append({\n                        \"name\": item,\n                        \"path\": full_path,\n                        \"type\": \"directory\",\n                        \"children\": children\n                    })\n            else:\n                _, ext = os.path.splitext(item)\n                if ext.lower() in allowed_extensions:\n                    structure.append({\n                        \"name\": item,\n                        \"path\": full_path,\n                        \"type\": \"file\"\n                    })\n    except Exception as e:\n        print(f\"Error reading directory {rootdir}: {e}\")\n\n    return structure\n\n\n\n@app.route('/open_folder', methods=['POST'])\ndef open_folder():\n    global selected_folder\n    global global_file_contents\n    global_file_contents = \"\"\n    folder = choose_folder()\n    if folder:\n        selected_folder = folder\n        structure = get_directory_structure(folder)\n        return jsonify({\"folder\": folder, \"structure\": structure})\n    else:\n        return jsonify({\"error\": \"No folder selected\"}), 400\n\n@app.route('/refresh', methods=['GET'])\ndef refresh():\n    global selected_folder\n    global global_file_contents\n    global_file_contents = \"\"\n    if selected_folder:\n        structure = get_directory_structure(selected_folder)\n        return jsonify({\"folder\": selected_folder, \"structure\": structure})\n    else:\n        return jsonify({\"error\": \"No folder selected\"}), 400\n\n@app.route('/load_selection', methods=['POST'])\ndef load_selection():\n    global selected_folder\n    global global_file_contents\n    global loaded_markdown_content\n    data = request.get_json()\n    selected_files = data.get(\"files\", [])\n    result = \"\"\n    if not selected_folder:\n        return jsonify({\"error\": \"No folder selected\"}), 400\n\n    for file in selected_files:\n        # Compute the relative path to the selected folder.\n        try:\n            relative_path = os.path.relpath(file, selected_folder)\n        except Exception as e:\n            relative_path = file  # Fallback to the full path if error occurs.\n        # Read file contents.\n        try:\n            with open(file, 'r', encoding='utf-8') as f:\n                content = f.read()\n        except Exception as e:\n            content = f\"Error reading file: {e}\"\n        # Append formatted block.\n        result += f\"---\\n\\nFILE: {relative_path}\\n\\n{content}\\n\\n\"\n    result += \"---\"\n    global_file_contents = result\n    return jsonify({\"result\": result})\n\n@app.route('/clear_loaded_content', methods=['POST'])\ndef clear_loaded_content():\n    \"\"\"\n    Clears the global file contents stored on the backend.\n    \"\"\"\n    global global_file_contents\n    global_file_contents = \"\"\n    return jsonify({\"message\": \"global_file_contents cleared\"}), 200\n\n\n\n\n# -----------------------------------\n# Dedicated state for index.html\n# -----------------------------------\n\n\n@app.route('/open_folder_chat', methods=['POST'])          # <‑‑ NEW ❷\ndef open_folder_chat():\n    \"\"\"\n    Folder picker used ONLY by templates/index.html\n    \"\"\"\n    global selected_folder_chat, global_file_contents\n    global_file_contents = \"\"\n    folder = choose_folder()\n    if folder:\n        selected_folder_chat = folder\n        return jsonify({\n            \"folder\": folder,\n            \"structure\": get_directory_structure(folder)\n        })\n    return jsonify({\"error\": \"No folder selected\"}), 400\n\n\n@app.route('/refresh_chat', methods=['GET'])               # <‑‑ NEW ❸\ndef refresh_chat():\n    \"\"\"\n    Refresh directory tree shown in templates/index.html\n    \"\"\"\n    global selected_folder_chat, global_file_contents\n    global_file_contents = \"\"\n    if selected_folder_chat:\n        return jsonify({\n            \"folder\": selected_folder_chat,\n            \"structure\": get_directory_structure(selected_folder_chat)\n        })\n    return jsonify({\"error\": \"No folder selected\"}), 400\n\n@app.route('/load_chat', methods=['GET'])\ndef load_chat():\n    \"\"\"\n    Called when the user clicks a file in the left‑hand browser that\n    is rendered inside templates/index.html.\n    \"\"\"\n\n    global selected_folder_chat\n    global loaded_conversation_messages\n    global loading_new_file\n    global loaded_markdown_content\n\n    # ---------- 1. figure out which path the client sent ----------\n    rel = request.args.get(\"rel\")        # relative path (recommended)\n    abs_path = request.args.get(\"abs\")   # absolute path (fallback)\n\n    if rel:\n        if not selected_folder_chat:\n            return jsonify({\"error\": \"No folder was opened in this session.\"}), 400\n        path = os.path.abspath(os.path.join(selected_folder_chat, rel))\n    elif abs_path:\n        path = os.path.abspath(abs_path)\n    else:\n        return jsonify({\"error\": \"Missing file path.\"}), 400\n\n    _, loaded_markdown_content = get_markdown_for_conversation(path)\n\n    # (optional) sanity‑check that the path is inside the chosen folder\n    if selected_folder_chat and not path.startswith(os.path.abspath(selected_folder_chat)):\n        return jsonify({\"error\": \"Path outside selected folder.\"}), 400\n\n    # ---------- 2. decide which JSON to open ----------\n    stem, ext = os.path.splitext(path)\n    if ext.lower() == \".md\":\n        conv_path = f\"{stem}messages.json\"\n    elif ext.lower() == \".txt\":\n        conv_path = f\"{stem}messages.json\"\n    elif ext.lower() == \".json\":\n        conv_path = path\n    else:\n        return jsonify({\"error\": \"Click a .md or .json file to load a conversation.\"}), 400\n\n    if not os.path.isfile(conv_path):\n        return jsonify({\"error\": f\"Conversation file not found: {conv_path}\"}), 404\n\n    # ---------- 3. load it ----------\n    try:\n        with open(conv_path, \"r\", encoding=\"utf-8\") as fh:\n            loaded_conversation_messages = json.load(fh)\n    except Exception as exc:  # noqa: BLE001  (keep traceback in log)\n        logger.error(\"Failed to load %s: %s\", conv_path, exc, exc_info=True)\n        return jsonify({\"error\": \"Failed to read conversation file.\"}), 500\n\n    loading_new_file = True\n    return render_template(\"index.html\")\n\n\n\ndef find_free(base=5000):\n    for p in range(base, 65536):\n        with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:\n            if s.connect_ex((\"127.0.0.1\", p)):\n                return p\n\nif __name__ == \"__main__\":\n    # decide the port only once, pass via env var\n    port = int(os.environ.get(\"DEV_PORT\", find_free()))\n    os.environ[\"DEV_PORT\"] = str(port)\n\n    print(f\"Serving on http://127.0.0.1:{port}\")\n    app.run(host=\"127.0.0.1\",\n            port=port,\n            debug=True,\n            use_reloader=True)      \n\n\n\n\n---",
    "role": "user"
  },
  {
    "content": "Below are three practical ways to get rid of the “&lt;pre&gt; … &lt;/pre&gt;” problem without giving up the rest of your tag–escaping logic.  \nPick the one that best matches your threat‑model and the amount of code you want to touch.\n\n────────────────────────────────────────\n1.  Stop using real HTML and switch to Markdown fences (easiest / safest)\n────────────────────────────────────────\nYou only wrap the loaded file in a `<pre>` block so that **its whole\ncontent is shown literally** in the preview.  \nMarkdown already has the perfect construct for that – a fenced code\nblock.\n\nReplace the line in the *get‑token* callback …\n\n```javascript\nmarkdownInput.value = \"<pre>\\n\" + sanitizedMarkdown + \"\\n</pre>\";\n```\n\nwith\n\n```javascript\n// pick a fence length that is longer than any run of back‑ticks\nconst fence = \"````\";          // 4 ticks because your sanitizer already\n                               // understands both 3 and 4 ticks\nmarkdownInput.value =\n  fence + \"markdown\\n\" +\n  sanitizedMarkdown +\n  \"\\n\" + fence;\n```\n\nBecause\n\n* `sanitizeText()` already skips everything that sits between\n  three‑or‑four back‑ticks,  \n* and `marked` converts the fence to a proper `<pre><code>` block,\n\nthe `<pre>` tag never appears in the source, so nothing gets escaped.\n\n────────────────────────────────────────\n2.  Allowlist the <pre> tag inside sanitizeText (one‑liner change)\n────────────────────────────────────────\nIf you really want literal `<pre>` tags to survive, teach the sanitiser\nto leave them alone.\n\nAdd an allow‑list just before you transform a found tag:\n\n```javascript\n// inside sanitizeInline …\ninlineParts[i] = inlineParts[i].replace(\n  /<\\s*(\\/?)\\s*([a-zA-Z][\\w-]*)([^>]*)>/g,\n  function (match, slash, tagName, rest) {\n\n    // ①  ———  allow “pre” and “/pre” unconditionally  ———\n    const tag = (slash ? \"/\" : \"\") + tagName.toLowerCase();\n    if (tag === \"pre\" || tag === \"/pre\") {\n      return match;               // keep the tag exactly as it is\n    }\n\n    // ②  ———  normal escaping for everything else  ———\n    …\n});\n```\n\nNothing else in the algorithm changes – you are just short‑circuiting\nthe replacement when the tag is `<pre>` or `</pre>`.\n\n────────────────────────────────────────\n3.  Preserve the whole <pre>…</pre> block with a placeholder (robust)\n────────────────────────────────────────\nIf you later decide to protect more HTML fragments\n(e.g. `<table>`, `<video>` …) you can wrap them with placeholders before\nsanitising and restore them afterwards:\n\n```javascript\nfunction sanitizeText(text) {\n\n  // A. protect every <pre …> … </pre> first\n  const PRE_RE   = /<pre\\b[^>]*>[\\s\\S]*?<\\/pre>/gi;\n  const store    = [];\n  text = text.replace(PRE_RE, m => {\n      const key = `___PRESERVE_PRE_${store.length}___`;\n      store.push(m);\n      return key;                  // leave a stub in the text\n  });\n\n  // B. run your existing algorithm (unchanged) on the stubbed text\n  text = coreSanitize(text);       // <‑‑ move your old code into here\n\n  // C. put the untouched <pre …> blocks back\n  store.forEach((block, i) => {\n      text = text.replace(`___PRESERVE_PRE_${i}___`, block);\n  });\n\n  return text;\n}\n```\n\nThis pattern lets you keep your escape‑all‑HTML default while still\nwhitelisting whole fragments whenever you need to.\n\n────────────────────────────────────────\nWhat the author of this project normally does\n────────────────────────────────────────\nIn production code we almost always pick option 1 (Markdown code fences)\nand run a real HTML sanitizer such as **DOMPurify** or **sanitize‑html**\nas a final guard before injecting anything into the DOM.  \nThat keeps XSS risk low and the code base small.\n\nBut if you are happy with your current regex‑based approach, option 2 is\nliterally a two‑line edit and solves the immediate problem.\n\nChoose whichever solution fits best – they are all compatible with the\nrest of your stack.",
    "role": "assistant"
  }
]